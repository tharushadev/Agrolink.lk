# -*- coding: utf-8 -*-
"""LocalDatasets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1svvZ4znbdzV2rO-vQJT7NrIW0uvRnvHQ
"""

import os
import pandas as pd
import numpy as np

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# ==========================================
# 1. FUNCTION TO CLEAN RAW DATASETS
# ==========================================
def clean_agrolink_file(filepath, variable_name, season_name):
    # Load file without header to handle the messy structure
    raw = pd.read_csv(filepath, header=None)

    # Forward fill the Year row (Row 1) to fix merged cells
    years_filled = raw.iloc[1, :].ffill()

    records = []
    # Row 2 contains 'Major', 'Minor', 'Total' headers
    # Row 3 onwards contains the data
    districts = raw.iloc[3:, 0].astype(str).str.strip().reset_index(drop=True)

    for col in range(1, raw.shape[1]):
        year_raw = years_filled[col]
        type_raw = raw.iloc[2, col]

        # We only want the "Total" columns
        if pd.notna(year_raw) and str(type_raw).strip().lower() == "total":
            # Clean Year (Handle "1978/1979" -> 1979)
            year_str = str(year_raw).strip()
            if "/" in year_str:
                year_val = int(year_str.split("/")[1]) # Take the harvest year
            else:
                try:
                    year_val = int(year_str)
                except: continue

            # Extract Values
            values = raw.iloc[3:, col]
            for district, val in zip(districts, values):
                val_str = str(val).replace(",", "").strip()
                if val_str not in ["-", "", "nan", "NaN", "."]:
                    try:
                        records.append({
                            "District": district,
                            "Year": year_val,
                            "Season": season_name,
                            variable_name: float(val_str)
                        })
                    except ValueError: continue

    return pd.DataFrame(records)

# ==========================================
# 2. PROCESS FILES
# ==========================================
# Load Yield Data
df_yield_maha = clean_agrolink_file(
    os.path.join(BASE_DIR, "Average_Yeild-Maha_Season-1979-2024.csv"),
    "Yield",
    "Maha"
)
df_yield_yala = clean_agrolink_file(
    os.path.join(BASE_DIR, "Average_Yeild-Yala_Season-1979-2024.csv"),
    "Yield",
    "Yala"
)
df_yield = pd.concat([df_yield_maha, df_yield_yala], ignore_index=True)

# Load Extent Data (Optional, but good for database)
df_extent_maha = clean_agrolink_file(
    os.path.join(BASE_DIR, "Harvested_Extent-Maha_Seasons-1979-2024.csv"),
    "Harvested_Extent",
    "Maha"
)

df_extent_yala = clean_agrolink_file(
    os.path.join(BASE_DIR, "Harvested_Extent-Yala_Seasons-1979-2024.csv"),
    "Harvested_Extent",
    "Yala"
)
df_extent = pd.concat([df_extent_maha, df_extent_yala], ignore_index=True)

# Merge into Master Dataset
df_master = pd.merge(df_yield, df_extent, on=["District", "Year", "Season"], how="outer")

# Filter out bad rows (Totals, empty districts)
df_master = df_master[~df_master["District"].isin(["nan", "Total", "Average yield: Kg per hectare"])]
df_master = df_master.dropna(subset=["District"])

# ==========================================
# 3. BUILD AI RISK MODEL
# ==========================================
# We use data from 2010-2024 to capture recent climate trends
recent_data = df_master[df_master["Year"] >= 2010].copy()

# Calculate Statistics: Mean Yield and Standard Deviation (Volatility)
risk_stats = recent_data.groupby(["District", "Season"])["Yield"].agg(["mean", "std"]).reset_index()

# AI Logic: Risk = Coefficient of Variation (StdDev / Mean)
# Higher Variation = Higher Risk
risk_stats["CV"] = risk_stats["std"] / risk_stats["mean"]

# Scale to 0-100 Score
# We assume a CV of 0.3 (30% fluctuation) is "Maximum Risk" for this scale
risk_stats["Risk_Score"] = (risk_stats["CV"] / 0.3) * 100
risk_stats["Risk_Score"] = risk_stats["Risk_Score"].clip(0, 100).round(1)

# Assign Labels
def get_label(score):
    if score < 25: return "Low Risk"
    elif score < 55: return "Medium Risk"
    else: return "High Risk"

risk_stats["Risk_Label"] = risk_stats["Risk_Score"].apply(get_label)

# ==========================================
# 4. SAVE OUTPUTS
# ==========================================
df_master.to_csv("AgroLink_Master_Dataset.csv", index=False)
risk_stats.to_csv("AgroLink_Risk_Scores.csv", index=False)

print("âœ… AI Module Complete!")
print("Risk Scores Preview:")
print(risk_stats.head())